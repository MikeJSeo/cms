---
title: "Causal Inference in Observational Epidemiology (Miguel Hernan)"
author: "Michael Seo"
output:
  html_document: default
  word_document: default
---

# Practical on marginal structural models

## R packages needed

```{r}
#install.packages("readstata13")
#install.packages("survival")
#install.packages("lmtest")
#install.packages("sandwich")

library(readstata13) #reading stata data
library(survival) # for cox regression
library(lmtest) #for robust standard errors
library(sandwich) #for robust standard errors
```

## Getting to know the data

The data for this practical come from a (hypothetical) HIV Cohort study. Let's first read in the dataset.

```{r}
# change the directory to where data is stored
setwd("C:/Users/ms19g661/Desktop/wengen causal inference course/data")
dat <- read.dta13("hivsetpract.dta")
```

With summary and structure function (i.e. str), you will get general information on the dataset.

```{r, eval = FALSE}
summary(dat)
str(dat)
```

Note, that the data is in long format. That means there are multiple records for the same person. To see that, sort the data by id and month and look at a few variables with the head command.

```{r}
# This is important; must run for future analysis
dat <- dat[order(dat$id, dat$month),]
#View(dat[1:300, c("id", "month", "cd4_v", "cd4_0", "ts_last_cd4", "art", "death", "censor")])
```

Look carefully at patients with id = 95001 and id = 95002.

With table and length function, you can see how many records and different patient id's you have.

```{r, eval = FALSE}
table(dat$id)
length(unique(dat$id))
```

The data contains certain information, like CD4 values at baseline, as a precise measurement (cd4_0) or as categories (cd4_0_cat). You can check how the categories were defined by, for example

```{r}
tapply(dat$cd4_0, dat$cd4_0_cat, summary)
```

## First a simple Cox proportional hazards model

* Actually this should be named time dependent cox proportional hazards model since the covariate value (i.e. art) changes with time. 

https://www.stata.com/manuals13/ststcox.pdf -> in this manual (pg 9), it is referred as Cox regression with discrete time-varying covariates

We could do a Cox proportional hazards analysis on that data set. We run a proportional hazards cox regression on the variable that denotes whether a person is on treatment (art)

```{r}
## note first that in this dataset when death is NA when censor = 1; thus we should treat these missing as a non-event.
#head(dat[is.na(dat$death),])

## since it is time varying, need to specify starting time (i.e. time) and ending timing (i.e. time2) of the interval. Data is simple since interval between months is not varying and fixed (1 month)
dat$month2 <- dat$month + 1
dat$SurvObj <- with(dat, Surv(time = month, time2 = month2, event = death == 1 & !is.na(death)))

## Fit Cox regression
simple.cox <- coxph(SurvObj ~ art, data = dat, id = id)
summary(simple.cox)

#Calculating the hazard ratio standard errors is less trivial
#See related link: #https://www.andrewheiss.com/blog/2016/04/25/convert-logistic-regression-standard-errors-to-odds-ratios-with-r/
#standard error of hazard ratio is then
summary(simple.cox)$coefficients[,"exp(coef)"] * sqrt(vcov(simple.cox))

#As an aside, there is a very good tutorial for survival analysis in R. 
#https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#example_data_-_the_bmt_dataset_from_semicomprisks_package
```

## Hazard ratio from pooled logistic regression

An alternative way to fit Cox models is via pooled logistic regression. Unlike Cox regression, in which the baseline hazard is not modelled, we have to model how hazard (the probability of the outcome in the next month) changes with time. We do that in our data set by incorporating month and month squared into the logistic regression. More generally one might use flexible splines on time.

```{r}
glm.model <- glm(death ~ art + month + monthsq, dat, family = binomial)
summary(glm.model)
exp(coef(glm.model)) #odds ratio

#Calculating the odds-ratio standard errors is again less trivial
exp(coef(glm.model)) * sqrt(diag(vcov(glm.model)))
```

Compare the results from the Cox regression and the pooled logistic regression.

Hazard ratio and 95% CI from Cox regression was:

Odds ratio and 95% CI from the logistic regression was:

Now obtain an estimated effect of ART when controlling for confounding by baseline variables, by including baseline variables in the logistic regression. You can use relevel function to specify a different reference.

```{r}
glm.model <- glm(death ~ art + month + monthsq + relevel(age_0_cat, ref = ">=50") + sex + relevel(origin, ref = "unknown") + relevel(year_0_cat, ref = ">=2004") + relevel(mode, ref = "other") + relevel(cd4_0_cat, ref = ">= 500") + relevel(rna_0_cat, ref = ">=100000 "), family = binomial, data = dat)
#summary(glm.model)
```

For comparison, we want to get an estimate when controlling for baseline and time updated variables.

```{r}
glm.model2 <- glm(death ~ art + month + monthsq + relevel(age_0_cat, ref = ">=50") + sex + relevel(origin, ref = "unknown") + relevel(year_0_cat, ref = ">=2004") + relevel(mode, ref = "other") + relevel(cd4_0_cat, ref = ">= 500") + relevel(rna_0_cat, ref = ">=100000 ")+ cd4_v + cd4_sq_v + relevel(rna_v_cat, ref = ">=100000 ") + aids, family = binomial, data = dat)
#summary(glm.model2)
```

# Deriving the weights for the marginal structural model in several steps

## Preparing treatment weights

First we need two logistic regressions for the denominator and numerator for calculating the stabilized inverse probability weights of starting ART. 

The denominator is based on a model for starting ART, including both baseline and time-updated covariates. Note, that we fit the model only for individuals not yet on ART. That means we restrict the analysis to individuals with the variable pastart equal to 0. We can use subset parameter in glm for this purpose.

```{r}
glm.model3 <- glm(art ~ month + monthsq + relevel(age_0_cat, ref = ">=50") + sex + relevel(origin, ref = "unknown") + relevel(year_0_cat, ref = ">=2004") + relevel(mode, ref = "other") + relevel(cd4_0_cat, ref = ">= 500") + relevel(rna_0_cat, ref = ">=100000 ")+ cd4_v + cd4_sq_v + relevel(rna_v_cat, ref = ">=100000 ") + aids, family = binomial, data = dat, subset = pastart == 0)
#summary(glm.model3)
```

In the next step we use the predict command to obtain the probability of starting ART at each time point. We restrict the results of the predict to the time points included in the logistic regression analysis. We use type == "response" to find the predicted probability. Also, if one is already on ART, the probability of continuing is by definition 1.

```{r}
pA_d <- rep(NA, dim(dat)[1])
pA_d[dat$pastart == 0] <- predict(glm.model3, type = "response")
pA_d[dat$pastart == 1] <- 1
```

Now we do the same three steps for the numerator by running a logistic regression on ART but only with the baseline variables:

```{r}
glm.model4 <- glm(art ~ month + monthsq + relevel(age_0_cat, ref = ">=50") + sex + relevel(origin, ref = "unknown") + relevel(year_0_cat, ref = ">=2004") + relevel(mode, ref = "other") + relevel(cd4_0_cat, ref = ">= 500") + relevel(rna_0_cat, ref = ">=100000 "), family = binomial, data = dat, subset = pastart == 0)
pA_n <- rep(NA, dim(dat)[1])
pA_n[dat$pastart == 0] <- predict(glm.model4, type = "response")
pA_n[dat$pastart == 1] <- 1
```

## Preparing censoring weights

Now we do the preparations for the censoring weights, again by running two logistic regression models. Here we need to be careful and note that the outcome in logistic regression is "not to be censored". This means that this is the complement of being censored.

```{r}
dat$notcensor <- 1 - dat$censor
glm.model5 <- glm(notcensor ~ month + monthsq + relevel(age_0_cat, ref = ">=50") + sex + relevel(origin, ref = "unknown") + relevel(year_0_cat, ref = ">=2004") + relevel(mode, ref = "other") + relevel(cd4_0_cat, ref = ">= 500") + relevel(rna_0_cat, ref = ">=100000 ")+ cd4_v + cd4_sq_v + relevel(rna_v_cat, ref = ">=100000 ") + aids, family = binomial, data = dat)
pC_d <- predict(glm.model5, type = "response")

glm.model6 <- glm(notcensor ~ month + monthsq + relevel(age_0_cat, ref = ">=50") + sex + relevel(origin, ref = "unknown") + relevel(year_0_cat, ref = ">=2004") + relevel(mode, ref = "other") + relevel(cd4_0_cat, ref = ">= 500") + relevel(rna_0_cat, ref = ">=100000 "), family = binomial, data = dat)
pC_n <- predict(glm.model6, type = "response")
```

## Putting all together

In the first step we need to take care that for the IPTW weights we need to use the probability of receiving the treatment a person actually received, and not just the probability of receiving ART. We will generate new variables, here called ptrgot_n and ptrgot_d by:

```{r}
ptrgot_n <- ifelse(dat$art == 1, pA_n, 1- pA_n)
ptrgot_d <- ifelse(dat$art == 1, pA_d, 1- pA_d)
```

Furthermore, we need to create the cumulative probability to have received what a person actually received, both for the denominator and numerator. This is achieved with the following:

```{r}
cum_ptrgot_n <- unlist(aggregate(ptrgot_n, by=list(dat$id) , FUN= cumprod)$x) 
cum_ptrgot_d <- unlist(aggregate(ptrgot_d, by=list(dat$id) , FUN= cumprod)$x)
```

Now generate stabilised and not stabilised IPTW weights

```{r}
swA <- cum_ptrgot_n / cum_ptrgot_d
wA <- 1/ cum_ptrgot_d
```

Now generate the cumulative probabilities not to be censored

```{r}
cum_pC_n <- unlist(aggregate(pC_n, by=list(dat$id) , FUN= cumprod)$x) 
cum_pC_d <- unlist(aggregate(pC_d, by=list(dat$id) , FUN= cumprod)$x)
swC <- cum_pC_n/cum_pC_d
wC <- 1/ cum_pC_d
```

Now we are ready to generate the final weights for the marginal structural model. We additionally truncate the stabilised weights at 10.

```{r}
sw <- swA * swC
w = wA * wC
sw[sw>10] <- 10
```

Before running the MSMS we check the distribution of weights we calculated.

```{r}
#summary(w[!is.na(dat$death)])
#summary(swA[!is.na(dat$death)])
#summary(swC[!is.na(dat$death)])
#summary(sw[!is.na(dat$death)])
```

Finally, let's fit the marginal structural model. This includes baseline but not time updated covariates, and is weighted using the stabilised weights. This is done with a weighted logistic regression now with the outcome death with stabilised weights and robust standard errors.

```{r}
remove(list = c("glm.model", "glm.model2", "glm.model3", "glm.model4", "glm.model5", "glm.model6"))
#https://stats.stackexchange.com/questions/117052/replicating-statas-robust-option-in-r
glm.model.final <- glm(death ~ art + month + monthsq + relevel(age_0_cat, ref = ">=50") + sex + relevel(origin, ref = "unknown") + relevel(year_0_cat, ref = ">=2004") + relevel(mode, ref = "other") + relevel(cd4_0_cat, ref = ">= 500") + relevel(rna_0_cat, ref = ">=100000 "), family = binomial, data = dat, weights = sw)
#coeftest(glm.model.final, vcov = vcovHC(glm.model.final, "HC1")) 

# calculating odds ratio
#exp(coeftest(glm.model.final, vcov = vcovHC(glm.model.final, "HC1"))[,"Estimate"]) 

# calculating the odds-ratio standard errors
#exp(coeftest(glm.model.final, vcov = vcovHC(glm.model.final, "HC1"))[,"Estimate"]) * sqrt(diag(vcovHC(glm.model.final, "HC1")))
```

Compare the results just obtained with those from a weighted logistic regression using the stabilised weights without robust standard errors.

```{r}
#summary(glm.model.final)

# calculating odds ratio
exp(coef(glm.model.final)) #odds ratio
exp(coef(glm.model.final)) * sqrt(diag(vcov(glm.model.final))) #odds-ratio standard errors
```

What is different?

How does the effect of ART compare to the effect estimated using standard methods?